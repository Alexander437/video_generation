# Video generation

Сервис для генерации видеоаватаров на основе текста.

[Таблица с проектами](https://docs.google.com/spreadsheets/d/1md-e4VOqdVASvk1Xtra2UEt0YHYCTLu_EQ_1jJcDvKk/edit?pli=1#gid=601294132)

## Roadmap

1. Существующие сервисы:
   - [Visper](https://visper.sberdevices.ru/en)
   - [Kvint.io](https://kvint.io)
   - Digital Production студии
   - [Synthesia](https://www.synthesia.io/) 
   - [D-ID](https://www.d-id.com/)

2. Сервис состоит из частей:
   - генерация лица
   - генерация речи
   - ...

3. Функциональность, преимущества/ограничения
   - ...

4. Пользовательский интерфейс

5. Deploy

6. Демовидео

## Источники

* https://github.com/Picsart-AI-Research/Text2Video-Zero
* https://huggingface.co/blog/text-to-video
* https://blog.syntha.ai/p/how-to-generate-video-from-a-text-prompt


1. AnimateDiff. Основной функционал этого подхода — генерация 
небольшого видео по тексту. Также есть множество вариаций AnimateDiff, 
в том числе и те, которые позволяют задать изначальную картинку, на основе 
которой будет генерироваться видео. Видео получаются довольно короткие. 

2. Библиотека huggingface diffusors. В ней реализован основной пайплайн 
AnimateDiff + множество дополнительных функций. 

3. OpenSora — хорошо генерирует статические видео по тексту. Если вы хотите 
разобраться с этим подходом, скорее всего вам понадобится разбираться в 
ресёрч-коде (в том числе и китайском, что не всегда просто).

4. GAN-based подходы. Такие подходы проще, чем диффузионные модели, но как 
правило они сразу заточены на решение конкретной задачи: например, lip sync 
(анимация рта говорящего), deepfake (наложение лица на готовое видео) и пр. 
Чтобы воспользоваться этим методом, вам необходимо под вашу задачу найти 
репозиторий на гитхабе с уже реализованной моделью.

попробуем отталкиваться от этого: https://github.com/OpenTalker/SadTalker

Добрый день!
Я хотел бы здесь обсудить некоторые мысли, чтобы понять, в правильном ли направлении двигаюсь и вовремя скорректировать, если это не так.

1. Видел модели для генерации речи по тексту (https://github.com/zxcq544/russian_text_to_speech)
2. Кажется в проектах типа Visper аватары нарисованы. Чтобы не рисовать аватаров можно попробовать генерировать видео, подавая на вход изображение (фото или нарисованного аватара). Это большой плюс, т.к. у пользователя гораздо больше свободы и возможностей, но и минус (вдруг будет работать не стабильно или кто-то будет делать deepfake)
3. Есть открытые модели для inpainting/vid2vid - если хоть какой-то аватар сгенерирован, можно попробовать его перерисовать
4. Остается вопрос с генерацией мимики и жестов по тексту/речи.
Ряд моделей генерирует только "говорящее лицо", приемлимый ли это результат?
Я продолжил искать модели для генерации не только мимики лица, но и жестов.
Похоже, что https://enriccorona.github.io/vlogger/paper.pdf - примерно то, что нужно, но нет в открытом доступе ни кода, ни набора данных.
Нашел похожий проект с набором данных - https://arxiv.org/abs/2212.04420, но там используются GAN, кажется диффузионные модели современнее.
Не знаю, стоит ли пробовать обучить модель с этим набором данных (+ наборы с "говорящей головой"), будет ли проблемой то, что в наборах данных отсутствует русский язык?

Буду благодарен за советы!