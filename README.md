# Video generation

Сервис для генерации видеоаватаров на основе текста.

[Таблица с проектами](https://docs.google.com/spreadsheets/d/1md-e4VOqdVASvk1Xtra2UEt0YHYCTLu_EQ_1jJcDvKk/edit?pli=1#gid=601294132)

## Steps

### 1. Анализ существующих сервисов

/* не претендую на абсолютную объективность, поправьте, если не прав

[Visper](https://visper.sberdevices.ru/en)

Плюсы:
- стабильная работа и высокое качество генерации
- возможность разместить презентацию или изображение, на фоне которого будет аватар
- поддержка ru/en
- чтение pdf

Минусы:
- частично предопределенные образы аватаров
- нет возможности для управления речью - скорректировать ударения, добавить паузы (хотя и без этого зачастую
хорошо, но все же что делать, если хочется по-другому)

[Kvint.io](https://kvint.io)

Плюсы:
- генерирует речь и отвечает на вопросы, а не требует пользовательского ввода
- обеспечивает живой диалог
- высокое качество генерации
 
Минусы:
- предопределенные образы аватаров

Digital Production студии
 

[Synthesia](https://www.synthesia.io/) 

Плюсы:
- стабильная работа и высокое качество генерации
- возможность разместить презентацию или изображение, на фоне которого будет аватар
- поддержка различных языков
- чтение pdf

Минусы:
- нет возможности для управления речью

[D-ID](https://www.d-id.com/)
Плюсы:
- возможность создания агентов с живым диалогом
- агенты могут ориентироваться на базы знаний
- высокое качество генерации

[Microsoft Lifelike](https://huggingface.co/posts/Jaward/597835329939130)
Плюсы:
- стабильная работа и высокое качество генерации
- высокая реалистичность аватаров

### 2. Части сервиса
   - генерация лица
   - генерация речи (используется [этот подход](https://colab.research.google.com/github/snakers4/silero-models/blob/master/examples_tts.ipynb))

### 3. Функциональность, преимущества/ограничения

Плюсы:
- генерация аватара, опираясь только на одно фото
- возможность корректировки речи

### 4. Пользовательский интерфейс

### 5. Deploy

### 6. Демовидео

## Источники

* https://github.com/Picsart-AI-Research/Text2Video-Zero
* https://huggingface.co/blog/text-to-video
* https://blog.syntha.ai/p/how-to-generate-video-from-a-text-prompt


1. AnimateDiff. Основной функционал этого подхода — генерация 
небольшого видео по тексту. Также есть множество вариаций AnimateDiff, 
в том числе и те, которые позволяют задать изначальную картинку, на основе 
которой будет генерироваться видео. Видео получаются довольно короткие. 

2. Библиотека huggingface diffusors. В ней реализован основной пайплайн 
AnimateDiff + множество дополнительных функций. 

3. OpenSora — хорошо генерирует статические видео по тексту. Если вы хотите 
разобраться с этим подходом, скорее всего вам понадобится разбираться в 
ресёрч-коде (в том числе и китайском, что не всегда просто).

4. GAN-based подходы. Такие подходы проще, чем диффузионные модели, но как 
правило они сразу заточены на решение конкретной задачи: например, lip sync 
(анимация рта говорящего), deepfake (наложение лица на готовое видео) и пр. 
Чтобы воспользоваться этим методом, вам необходимо под вашу задачу найти 
репозиторий на гитхабе с уже реализованной моделью.

попробуем отталкиваться от этого: https://github.com/OpenTalker/SadTalker

Добрый день!
Я хотел бы здесь обсудить некоторые мысли, чтобы понять, в правильном ли направлении двигаюсь и вовремя скорректировать, если это не так.

1. Видел модели для генерации речи по тексту (https://github.com/zxcq544/russian_text_to_speech)
2. Кажется в проектах типа Visper аватары нарисованы. Чтобы не рисовать аватаров можно попробовать генерировать видео, подавая на вход изображение (фото или нарисованного аватара). Это большой плюс, т.к. у пользователя гораздо больше свободы и возможностей, но и минус (вдруг будет работать не стабильно или кто-то будет делать deepfake)
3. Есть открытые модели для inpainting/vid2vid - если хоть какой-то аватар сгенерирован, можно попробовать его перерисовать
4. Остается вопрос с генерацией мимики и жестов по тексту/речи.
Ряд моделей генерирует только "говорящее лицо", приемлимый ли это результат?
Я продолжил искать модели для генерации не только мимики лица, но и жестов.
Похоже, что https://enriccorona.github.io/vlogger/paper.pdf - примерно то, что нужно, но нет в открытом доступе ни кода, ни набора данных.
Нашел похожий проект с набором данных - https://arxiv.org/abs/2212.04420, но там используются GAN, кажется диффузионные модели современнее.
Не знаю, стоит ли пробовать обучить модель с этим набором данных (+ наборы с "говорящей головой"), будет ли проблемой то, что в наборах данных отсутствует русский язык?

Буду благодарен за советы!